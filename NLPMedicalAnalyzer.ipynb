{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/isaashka/NLPMedicalAnalyzer/blob/main/NLPMedicalAnalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3CLxyK4kjm0"
   },
   "source": [
    "# Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 13:58:50.944 Python[34192:419801] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FzF6mi4KbJWs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific imports\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSDZHGxxbS4O"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class\n",
    "\n",
    "class Tokenizer:\n",
    "  def __init__(self, lowercase=False):\n",
    "    self.lowercase = lowercase  # If this is True, convert text to lowercase while tokenizing.\n",
    "    self.vocab = []\n",
    "\n",
    "  def tokenize(self, string):\n",
    "    tokens = word_tokenize(string)\n",
    "    self.vocab += [w for w in set(tokens) if w not in self.vocab]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#User input\n",
    "tokenizer = Tokenizer(lowercase = True) \n",
    "user_input = input(\"Tell me about your symptoms: \")\n",
    "\n",
    "tokens = tokenizer.tokenize(user_input)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot\n",
    "def normalize(tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example usage\n",
    "words = ['running', 'swimming', 'better', 'children', 'easily', 'faster']\n",
    "\n",
    "    normalized_input = correct_spelling(normalized_input)\n",
    "    # print(normalized_input)\n",
    "    return(normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i have been having migraines and headaches i cannot sleep my whole body is shaking and shivering i feel dizzy sometimes'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_symptoms = user_input()\n",
    "user_symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ngram approach\n",
    "\n",
    "\n",
    "Test input: I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symptom_phrases(tokens, symptom_keywords, body_parts, n=2):\n",
    "    symptom_phrases = []\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "\n",
    "    for ngram in ngrams_list:\n",
    "        for keyword in symptom_keywords:\n",
    "            for body_part in body_parts:\n",
    "                if keyword in ngram and body_part in ngram:\n",
    "                    symptom_phrases.append(' '.join(ngram))\n",
    "\n",
    "    return symptom_phrases\n",
    "\n",
    "# Lemmatize input\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = []  \n",
    "    for token in tokens:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def tokenize_user_symptoms():\n",
    "    print(\"Enter symptoms: \")\n",
    "    \n",
    "    symptoms = input()\n",
    "    # tokenize input\n",
    "    symptom_tokens = word_tokenize(symptoms)\n",
    "\n",
    "    # lemmatize input\n",
    "    symptom_tokens_lemmatized = lemmatize_tokens(symptom_tokens)\n",
    "\n",
    "    # find symptom phrases using n-grams\n",
    "    symptom_phrases = find_symptom_phrases(symptom_tokens_lemmatized, \\\n",
    "                                           dataset_symptoms, dataset_symptoms) # dataset_symptoms = symptoms retrieved from the dataset\n",
    "    \n",
    "    return symptom_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extracted_symptoms \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_user_symptoms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms your list of symptoms: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mtokenize_user_symptoms\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_user_symptoms\u001b[39m():\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter symptoms: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     symptoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# tokenize input\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     symptom_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(symptoms)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "extracted_symptoms = tokenize_user_symptoms()\n",
    "print()\n",
    "print('Here\\'s your list of symptoms: ')\n",
    "print(extracted_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms: ['mig', '##raine', 'headache', 'cannot', 'sleep', 'shaking', 'shivering', 'dizzy', 'fever']\n",
      "Keywords: ['migraine', 'headache', 'cannot', 'sleep', 'shaking', 'shivering', 'dizzy', 'fever']\n"
     ]
    }
   ],
   "source": [
    "# Example user input\n",
    "start_user_input = \"I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes. I have a fever.\"\n",
    "new_user_input = normalize_user_input(start_user_input)\n",
    "\n",
    "# Extract symptoms\n",
    "symptoms = extract_symptoms(new_user_input)\n",
    "\n",
    "# Clean and join the tokens into keywords\n",
    "keywords = clean_keywords(symptoms)\n",
    "\n",
    "# Display the symptoms\n",
    "print(\"Extracted Symptoms:\", symptoms)\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand list of symptoms by finding synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# find synonyms for extracted keywords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms of a list of words\n",
    "def get_synonyms(words):\n",
    "    synonyms = set()\n",
    "    for word in words:\n",
    "        # Get synsets (sets of synonyms) for each word\n",
    "        synsets = wordnet.synsets(word)\n",
    "        for synset in synsets:\n",
    "            # Add synonyms of the word from each synset to the set\n",
    "            synonyms.update([lemma.name() for lemma in synset.lemmas()])\n",
    "    return synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym set: \n",
      "{'lightheaded', 'kip', 'nap', 'chill', 'rock', 'pyrexia', 'shake_off', 'quivering', 'thrill', 'throb', 'sick_headache', 'cephalalgia', 'quietus', 'vertiginous', 'rest', 'worry', 'shiver', 'febricity', 'sopor', 'empty-headed', 'silly', 'vexation', \"log_Z's\", 'airheaded', 'judder', 'sway', 'throw_off', 'shaky', 'agitate', 'shivering', 'feverishness', 'shudder', 'febrility', 'sleep', 'shaking', 'didder', 'stir', 'giddy', 'headache', 'palpitation', 'eternal_rest', 'excite', 'fever', 'migraine', 'hemicrania', \"catch_some_Z's\", 'shake', 'quiver', 'trembling', 'woozy', 'concern', 'megrim', 'vibration', 'light-headed', 'featherbrained', 'dizzy', 'slumber', 'shake_up', 'escape_from', 'eternal_sleep', 'stimulate', 'shakiness', 'head_ache'}\n",
      "\n",
      "List of all symptoms: \n",
      "['lightheaded', 'kip', 'nap', 'chill', 'rock', 'pyrexia', 'shake_off', 'quivering', 'thrill', 'throb', 'sick_headache', 'cephalalgia', 'quietus', 'vertiginous', 'rest', 'worry', 'shiver', 'febricity', 'sopor', 'empty-headed', 'silly', 'vexation', \"log_Z's\", 'airheaded', 'judder', 'sway', 'throw_off', 'shaky', 'agitate', 'shivering', 'feverishness', 'shudder', 'febrility', 'sleep', 'shaking', 'didder', 'stir', 'giddy', 'headache', 'palpitation', 'eternal_rest', 'excite', 'fever', 'migraine', 'hemicrania', \"catch_some_Z's\", 'shake', 'quiver', 'trembling', 'woozy', 'concern', 'megrim', 'vibration', 'light-headed', 'featherbrained', 'dizzy', 'slumber', 'shake_up', 'escape_from', 'eternal_sleep', 'stimulate', 'shakiness', 'head_ache']\n"
     ]
    }
   ],
   "source": [
    "synonyms = get_synonyms(keywords)\n",
    "print(\"Synonym set: \")\n",
    "print(synonyms)\n",
    "print()\n",
    "\n",
    "print(\"List of all symptoms: \")\n",
    "final_symptoms = list(synonyms)\n",
    "print(final_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the closest match using Levenshtein distance for final list of symptoms compared to the dataset symptoms\n",
    "# currently unimplemented as we haven't found a good way to make it work\n",
    "def find_closest_match(word):\n",
    "    closest_match = min(dataset_symptoms, key=lambda w: Levenshtein.distance(word, w))\n",
    "    return closest_match\n",
    "\n",
    "# Function to extract relevant symptoms based on Levenshtein distance\n",
    "def extract_relevant_symptoms(final_symptoms):\n",
    "    relevant_symptoms = []\n",
    "    for sym in final_symptoms:\n",
    "        # closest_match = find_closest_match(sym)\n",
    "        if sym in dataset_symptoms:\n",
    "            relevant_symptoms.append(sym)\n",
    "        # relevant_symptoms.append(closest_match)\n",
    "    return relevant_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chill', 'shivering', 'shaking', 'headache', 'fever', 'shakiness']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_relevant_symptoms(final_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible future implementations for Preprocessing:\n",
    "\n",
    "Look for symptom phrases to develop / inference a more descriptive symptom. Some keywords may not mean much on their own, but the words around them give more meaning. For example the word \"sleep,\" the complete phrase may be \"I can't sleep\" or \"I have trouble sleeping.\" To get the full picture we can look at words preceding and following the symptom keyword. \n",
    "\n",
    "_ _ _ sleep\n",
    "\n",
    "sleep _ _ _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting different forms of the symptom keywords\n",
    "\n",
    "We can increase chances of finding the right symptoms just by changing the form of the word used. Ex. \"sleepy\" to \"sleepiness\". But there are currently no libraries that can do this accurately and coding this up would take a lot of time which is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sleepe',\n",
       " 'sleeped',\n",
       " 'sleepes',\n",
       " 'sleeping',\n",
       " 'sleepful',\n",
       " 'sleepness',\n",
       " 'sleepfulness']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all forms of the extracted symptoms\n",
    "# i.e. 'sleep' --> 'sleepy', 'sleepiness', 'sleep'\n",
    "#\n",
    "# !pip install reversestem\n",
    "\n",
    "from reversestem import unstem\n",
    "\n",
    "unstem('sleep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Models\n",
    "\n",
    "Classification models used to match an input list of symptoms to the disease.\n",
    "\n",
    "Following code inspired by this github repo: https://github.com/rahul15197/Disease-Detection-based-on-Symptoms?source=post_page-----54e6be60a3d1--------------------------------\n",
    "\n",
    "We used the dataset from the github repo as well as some general structure of code. Most of the code was written by us to suit our overall structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists used for performance plots\n",
    "\n",
    "model_list = []\n",
    "f1_scores = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "symptoms_dataset = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "\n",
    "# separate the features and labels\n",
    "X = symptoms_dataset.iloc[:, 1:]\n",
    "Y = symptoms_dataset.iloc[:, 0:1]\n",
    "\n",
    "# convert Y (labels) to a 1D array \n",
    "y_array = Y.to_numpy() if isinstance(Y, pd.Series) else Y.values\n",
    "Y = y_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.10)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7951, 489)\n",
      "(7951,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)  # Check the shape of X\n",
    "print(y_train.shape)  # Check the shape of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model according to its type and make predictions\n",
    "def fit_and_predict(classifier, model_type):\n",
    "    classifier = classifier.fit(X, Y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    precision, recall, f1, accuracy = evaluate_model(y_test, y_pred, model_type)\n",
    "    print_evaluation_metrics(precision, recall, f1, accuracy)\n",
    "\n",
    "# Evaluate the model on different metrics\n",
    "def evaluate_model(y_test, y_pred, model_type):\n",
    "    # Calculate precision, recall, and F1-score for each class and then compute the macro average\n",
    "    precision = round(Decimal(precision_score(y_test, y_pred, average='macro', zero_division=1) * 100), 2)\n",
    "    recall= round(Decimal(recall_score(y_test, y_pred, average='macro', zero_division=1) * 100), 2)\n",
    "    f1= round(Decimal(f1_score(y_test, y_pred, average='macro') * 100), 2)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = round(Decimal(accuracy_score(y_test, y_pred) * 100), 2)\n",
    "    \n",
    "    # Add metrics to overall arrays used later for graphing\n",
    "    model_list.append(model_type)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "    \n",
    "def print_evaluation_metrics(precision, recall, f1, accuracy):\n",
    "    print(f'Macro-average Precision: {precision}%')\n",
    "    print(f'Macro-average Recall: {recall}%')\n",
    "    print(f'Macro-average F1-score: {f1}%')\n",
    "    print(f'Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints all model names and their f1 score and accuracy\n",
    "def print_all_metrics():\n",
    "    for i in range(len(model_list)):\n",
    "        print(f'Model: {model_list[i]}, f1 = {f1_scores[i]} accuracy = {accuracies[i]}')\n",
    "        # print(model_list[i], f1_scores[i], accuracies[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "fit_and_predict(mnb, 'MultinomialNaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "fit_and_predict(svm, 'SupportVectorMachine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine classifier\n",
    "lr = LogisticRegression()\n",
    "fit_and_predict(lr, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric lists for reference\n",
    "# model_list = []\n",
    "# f1_scores = []\n",
    "# accuracies = []\n",
    "\n",
    "# Comparison plot for all classifiers with their accuracy\n",
    "plt.style.use('_classic_test_patch')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "plt.title(\"Model Vs Accuracy\", color='black', pad=30)\n",
    "plt.xlabel('Classifier', color='black')\n",
    "plt.ylabel('Accuracy (%)', color='black')\n",
    "plt.bar(model_list, accuracies, color='lightblue')\n",
    "for i, j in enumerate(accuracies):\n",
    "    ax.text(float(i)-0.15, float(j)+0.7, str(j), color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison plot for all classifiers with their F1-score\n",
    "plt.style.use('_classic_test_patch')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "plt.title(\"Model Vs F1-score\", color='black', pad=30)\n",
    "plt.xlabel('Classifier', color='black')\n",
    "plt.ylabel('F1-score (%)', color='black')\n",
    "plt.bar(model_list, f1_scores, color='lightblue')\n",
    "for i, j in enumerate(f1_scores):\n",
    "    ax.text(float(i)-0.15, float(j)+0.7, str(j), color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedicalAnalyzer for Disease Output\n",
    "\n",
    "This is the main user-model interactive section, all the above code needs to be run so that the following works. This is the minimalist and to the point interaction between user and our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "The best performing model was Logistic Regression, so we'll use that in our evaluations of user input.\n",
    "\n",
    "This code almost completely adapted from: https://github.com/rahul15197/Disease-Detection-based-on-Symptoms/blob/master/SymptomSuggestion.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_comb = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "df_norm = pd.read_csv(\"Dataset/dis_sym_dataset_norm.csv\")\n",
    "\n",
    "# separate the features and labels\n",
    "X = df_comb.iloc[:, 1:]\n",
    "Y = df_comb.iloc[:, 0:1]\n",
    "\n",
    "# convert Y (labels) to a 1D array \n",
    "y_array = Y.to_numpy() if isinstance(Y, pd.Series) else Y.values\n",
    "Y = y_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X, Y)\n",
    "scores = cross_val_score(lr, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_norm.iloc[:, 1:]\n",
    "Y = df_norm.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of symptoms\n",
    "dataset_symptoms = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Symptoms --> Disease\n",
    "\n",
    "This is where we use the model to tell us what diseases the user may have based on the symptoms extracted from their input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector from processed user symptoms to be used by the model\n",
    "\n",
    "# example symptom list\n",
    "# sym_list = [\"yellowish skin\",\"wheezing\",\"abdominal cramp\",\"back\",\"feeling cold\"]\n",
    "\n",
    "def create_symptom_vector(sym_list):\n",
    "    sym_vector = [0 for i in range(0, len(dataset_symptoms))]\n",
    "    for sym in sym_list:\n",
    "        # print(sym)\n",
    "        sym_vector[dataset_symptoms.index(sym)] = 1\n",
    "    return sym_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the likelihood for each disease\n",
    "diseases = list(set(Y['label_dis']))\n",
    "diseases.sort()\n",
    "def predict_diseases(sym_vector):\n",
    "    disease_predict = lr.predict_proba([sym_vector])\n",
    "    k = 10\n",
    "    top_k = disease_predict[0].argsort()[-k:][::-1]\n",
    "    return top_k, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates top k diseases that match the symptom list vector and prints out the name of the disease along with how likely it is that disease\n",
    "\n",
    "def output_top_k_diseases(top_k, k, sym_list):\n",
    "    print(f\"\\nTop {k} diseases predicted based on symptoms\")\n",
    "    topk_dict = {}\n",
    "    # Show top 10 highly probable disease to the user.\n",
    "    for idx,t in  enumerate(top_k):\n",
    "        match_sym=set()\n",
    "        row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
    "        row[0].pop(0)\n",
    "    \n",
    "        for idx,val in enumerate(row[0]):\n",
    "            if val!=0:\n",
    "                match_sym.add(dataset_symptoms[idx])\n",
    "        prob = (len(match_sym.intersection(set(sym_list)))+1)/(len(set(sym_list))+1)\n",
    "        prob *= np.mean(scores)\n",
    "        topk_dict[t] = prob\n",
    "    j = 0\n",
    "    topk_index_mapping = {}\n",
    "    topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    for key in topk_sorted:\n",
    "        prob = topk_sorted[key]*100\n",
    "\n",
    "        # number_dis = f\"{str(j+1):<2}\"\n",
    "        # disease_name = f\"{diseases[key]:<10}\"\n",
    "        # probability_of_dis = f\"{str(round(prob, 2)):<30}\"\n",
    "        # # prob = f\"Probability::<20\"\n",
    "        \n",
    "        print(str(j+1) + \" Disease name:\",diseases[key], \"\\tProbability:\",str(round(prob, 2))+\"%\")\n",
    "     \n",
    "        topk_index_mapping[j] = key\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-interactable Interface\n",
    "\n",
    "Note: In actual implementation the user wouldn't get the list of diseases shown to them, it would instead be saved to a file and shown to the doctor. This implementation is created to show all of the output in one place to see how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "# user_input = \"I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes. I have a fever.\"\n",
    "# new_user_input = normalize_user_input(user_input)\n",
    "def MedicalAnalyzer():\n",
    "    new_user_input = user_input()\n",
    "    print()\n",
    "    \n",
    "    # Extract symptoms\n",
    "    symptoms = extract_symptoms(new_user_input)\n",
    "    \n",
    "    # Clean and join the tokens into keywords\n",
    "    keywords = clean_keywords(symptoms)\n",
    "    \n",
    "    # Display the symptoms\n",
    "    print(\"Key symptom words: \", keywords)\n",
    "    print()\n",
    "    \n",
    "    # Display all symptoms including synonyms\n",
    "    synonyms = get_synonyms(keywords)\n",
    "    final_symptoms = list(synonyms)\n",
    "    print(\"Symptoms synonyms list: \", final_symptoms)\n",
    "    print()\n",
    "    \n",
    "    # Get relevant symptoms\n",
    "    relevant_symptoms = extract_relevant_symptoms(final_symptoms)\n",
    "    \n",
    "    print(\"Final symptoms list: \", relevant_symptoms)\n",
    "    print()\n",
    "    \n",
    "    # Create a vector to be used by the model\n",
    "    sym_vector = create_symptom_vector(relevant_symptoms)\n",
    "    \n",
    "    # Get top diseases\n",
    "    topk_diseases, k = predict_diseases(sym_vector)\n",
    "    \n",
    "    output_top_k_diseases(topk_diseases, k, relevant_symptoms)\n",
    "# Hi doctor, I am a 26 year old male. I am 5 feet and 9 inches tall and weigh 255 pounds. When I eat spicy food, I poop blood. Sometimes when I have constipation as well, I poop a little bit of blood. I am really scared that I have colon cancer. I do have diarrhea often. I do not have a family history of colon cancer. I got blood tests done last night. Please find my reports attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hi doctor, I am a 26 year old male. I am 5 feet and 9 inches tall and weigh 255 pounds. When I eat spicy food, I poop blood. Sometimes when I have constipation as well, I poop a little bit of blood. I am really scared that I have colon cancer. I do have diarrhea often. I do not have a family history of colon cancer. I got blood tests done last night. Please find my reports attached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key symptom words:  ['constipation', 'scared', 'diarrhea', 'constipation', 'diarrhea']\n",
      "\n",
      "Symptoms synonyms list:  ['constipation', 'stultification', 'looseness_of_the_bowels', 'impairment', 'affright', 'irregularity', 'frightened', 'scare', 'daunt', 'scare_off', 'mark', 'scar', 'diarrhea', 'fright', 'frighten', 'pit', 'frighten_away', 'dash', 'diarrhoea', 'scare_away', 'pall', 'looseness', 'deadening', 'scared', 'pock', 'frighten_off']\n",
      "\n",
      "Final symptoms list:  ['constipation', 'diarrhea', 'diarrhoea']\n",
      "\n",
      "\n",
      "Top 10 diseases predicted based on symptoms\n",
      "1 Disease name: Celiacs disease \tProbability: 66.89%\n",
      "2 Disease name: Irritable bowel syndrome \tProbability: 66.89%\n",
      "3 Disease name: Ebola \tProbability: 44.6%\n",
      "4 Disease name: Hyperthyroidism \tProbability: 44.6%\n",
      "5 Disease name: Porphyria \tProbability: 44.6%\n",
      "6 Disease name: Lead poisoning \tProbability: 44.6%\n",
      "7 Disease name: Hypothyroid \tProbability: 44.6%\n",
      "8 Disease name: Anthrax \tProbability: 44.6%\n",
      "9 Disease name: lactose intolerance \tProbability: 44.6%\n",
      "10 Disease name: Crimean Congo haemorrhagic fever (CCHF) \tProbability: 44.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MedicalAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
