{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/isaashka/NLPMedicalAnalyzer/blob/main/NLPMedicalAnalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3CLxyK4kjm0"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 15:04:23.060 Python[27475:135143] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FzF6mi4KbJWs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific imports\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSDZHGxxbS4O"
   },
   "source": [
    "# Data Preprocessing\n",
    "### Goal: convert user input --> symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "symptoms_dataset = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "\n",
    "# extract features to use as a symptom array\n",
    "X = symptoms_dataset.iloc[:, 1:]\n",
    "dataset_symptoms = list(X.columns)\n",
    "# dataset_symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import re\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input():\n",
    "    print(\"Enter symptoms: \")\n",
    "    user_input = input()\n",
    "    return normalize_user_input(user_input)\n",
    "\n",
    "def correct_spelling(user_input):\n",
    "    spell = SpellChecker()\n",
    "    words = user_input.split()\n",
    "    corrected_words = [spell.correction(word) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def normalize_user_input(user_input):\n",
    "    # lowercase\n",
    "    normalized_input = user_input.lower()\n",
    "    # fix contractions\n",
    "    normalized_input = contractions.fix(normalized_input)\n",
    "    # get rid of punctuation\n",
    "    normalized_input = re.sub(r'[^\\w\\s]', '', normalized_input)\n",
    "\n",
    "    normalized_input = correct_spelling(normalized_input)\n",
    "    # print(normalized_input)\n",
    "    return(normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i have been having migraines and headaches i cannot sleep my whole body is shaking and shivering i feel dizzy sometimes'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_symptoms = user_input()\n",
    "user_symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram approach\n",
    "\n",
    "Test input: I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symptom_phrases(tokens, symptom_keywords, body_parts, n=2):\n",
    "    symptom_phrases = []\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "\n",
    "    for ngram in ngrams_list:\n",
    "        for keyword in symptom_keywords:\n",
    "            for body_part in body_parts:\n",
    "                if keyword in ngram and body_part in ngram:\n",
    "                    symptom_phrases.append(' '.join(ngram))\n",
    "\n",
    "    return symptom_phrases\n",
    "\n",
    "# Lemmatize input\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = []  \n",
    "    for token in tokens:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def tokenize_user_symptoms():\n",
    "    print(\"Enter symptoms: \")\n",
    "    \n",
    "    symptoms = input()\n",
    "    # tokenize input\n",
    "    symptom_tokens = word_tokenize(symptoms)\n",
    "\n",
    "    # lemmatize input\n",
    "    symptom_tokens_lemmatized = lemmatize_tokens(symptom_tokens)\n",
    "\n",
    "    # find symptom phrases using n-grams\n",
    "    symptom_phrases = find_symptom_phrases(symptom_tokens_lemmatized, \\\n",
    "                                           dataset_symptoms, dataset_symptoms) # dataset_symptoms = symptoms retrieved from the dataset\n",
    "    \n",
    "    return symptom_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's your list of symptoms: \n",
      "['and headache', 'headache .', 'is shaking', 'shaking and', 'and shivering', 'shivering .']\n"
     ]
    }
   ],
   "source": [
    "extracted_symptoms = tokenize_user_symptoms()\n",
    "print()\n",
    "print('Here\\'s your list of symptoms: ')\n",
    "print(extracted_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER / BERT approach --- WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages\n",
    "!pip install transformers pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained NER model that's specialized for biomedical text\n",
    "ner = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\")\n",
    "\n",
    "# Load the symptoms dataset\n",
    "symptoms_dataset = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "\n",
    "# Extract the list of symptoms from the dataset\n",
    "# Assuming the symptoms are in a column named 'symptoms'\n",
    "# symptoms_list = symptoms_dataset['symptom'].str.lower().tolist()\n",
    "\n",
    "def extract_symptoms(text):\n",
    "    # Use the NER pipeline to extract entities\n",
    "    entities = ner(text)\n",
    "    \n",
    "    # Print the entities for debugging\n",
    "    # print(\"Entities:\", entities)\n",
    "    \n",
    "    # Extract symptom-related entities detected by the model\n",
    "    symptoms = [entity['word'] for entity in entities if 'symptom' in entity['entity'].lower()]\n",
    "    \n",
    "    # Tokenize the text for manual matching\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Add manually matched symptoms from the dataset\n",
    "    for token in tokens:\n",
    "        if token in dataset_symptoms and token not in symptoms:\n",
    "            symptoms.append(token)\n",
    "    \n",
    "    return symptoms\n",
    "\n",
    "# Function to clean and join tokens into keywords\n",
    "def clean_keywords(keywords):\n",
    "    clean_tokens = []\n",
    "    for token in keywords:\n",
    "        if token.startswith(\"##\"):\n",
    "            clean_tokens[-1] = clean_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms: ['mig', '##raine', 'headache', 'cannot', 'sleep', 'shaking', 'shivering', 'dizzy', 'fever']\n",
      "Keywords: ['migraine', 'headache', 'cannot', 'sleep', 'shaking', 'shivering', 'dizzy', 'fever']\n"
     ]
    }
   ],
   "source": [
    "# Example user input\n",
    "user_input = \"I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes. I have a fever.\"\n",
    "new_user_input = normalize_user_input(user_input)\n",
    "\n",
    "# Extract symptoms\n",
    "symptoms = extract_symptoms(new_user_input)\n",
    "\n",
    "# Clean and join the tokens into keywords\n",
    "keywords = clean_keywords(symptoms)\n",
    "\n",
    "# Display the symptoms\n",
    "print(\"Extracted Symptoms:\", symptoms)\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand list of symptoms by finding synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# find synonyms for extracted keywords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms of a list of words\n",
    "def get_synonyms(words):\n",
    "    synonyms = set()\n",
    "    for word in words:\n",
    "        # Get synsets (sets of synonyms) for each word\n",
    "        synsets = wordnet.synsets(word)\n",
    "        for synset in synsets:\n",
    "            # Add synonyms of the word from each synset to the set\n",
    "            synonyms.update([lemma.name() for lemma in synset.lemmas()])\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym set: \n",
      "{'throw_off', 'migraine', 'kip', 'shake_off', 'judder', 'lightheaded', 'headache', 'hemicrania', 'sleep', 'sick_headache', 'sway', 'head_ache', 'light-headed', 'megrim', 'didder', 'quivering', 'dizzy', 'quiver', 'fever', 'cephalalgia', 'escape_from', 'silly', 'throb', 'woozy', 'palpitation', 'vexation', \"catch_some_Z's\", 'trembling', 'rest', 'eternal_rest', 'shake_up', 'febrility', 'chill', 'empty-headed', 'stimulate', 'vertiginous', 'eternal_sleep', 'shakiness', 'nap', 'featherbrained', 'pyrexia', 'shaking', 'giddy', 'shudder', 'rock', 'quietus', 'shivering', 'airheaded', 'stir', 'thrill', 'sopor', 'febricity', 'concern', 'agitate', 'shaky', 'excite', 'shiver', \"log_Z's\", 'worry', 'vibration', 'slumber', 'shake', 'feverishness'}\n",
      "\n",
      "List of all symptoms: \n",
      "['throw_off', 'migraine', 'kip', 'shake_off', 'judder', 'lightheaded', 'headache', 'hemicrania', 'sleep', 'sick_headache', 'sway', 'head_ache', 'light-headed', 'megrim', 'didder', 'quivering', 'dizzy', 'quiver', 'fever', 'cephalalgia', 'escape_from', 'silly', 'throb', 'woozy', 'palpitation', 'vexation', \"catch_some_Z's\", 'trembling', 'rest', 'eternal_rest', 'shake_up', 'febrility', 'chill', 'empty-headed', 'stimulate', 'vertiginous', 'eternal_sleep', 'shakiness', 'nap', 'featherbrained', 'pyrexia', 'shaking', 'giddy', 'shudder', 'rock', 'quietus', 'shivering', 'airheaded', 'stir', 'thrill', 'sopor', 'febricity', 'concern', 'agitate', 'shaky', 'excite', 'shiver', \"log_Z's\", 'worry', 'vibration', 'slumber', 'shake', 'feverishness']\n"
     ]
    }
   ],
   "source": [
    "synonyms = get_synonyms(keywords)\n",
    "print(\"Synonym set: \")\n",
    "print(synonyms)\n",
    "print()\n",
    "\n",
    "print(\"List of all symptoms: \")\n",
    "final_symptoms = list(synonyms)\n",
    "print(final_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
      "  Downloading Levenshtein-0.25.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.9.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.25.1-cp310-cp310-macosx_10_9_x86_64.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m566.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.9.3-cp310-cp310-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.25.1 python-Levenshtein-0.25.1 rapidfuzz-3.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headache', 'fever', 'chill', 'shakiness', 'shaking', 'shivering']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_relevant_symptoms(final_symptoms):\n",
    "    syms_in_dataset = []\n",
    "    for sym in final_symptoms:\n",
    "        if sym in dataset_symptoms:\n",
    "            syms_in_dataset.append(sym)\n",
    "            # print(sym)\n",
    "    return syms_in_dataset\n",
    "    \n",
    "extract_relevant_symptoms(final_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible future implementations for Preprocessing:\n",
    "\n",
    "Look for symptom phrases to develop / inference a more descriptive symptom. Some keywords may not mean much on their own, but the words around them give more meaning. For example the word \"sleep,\" the complete phrase may be \"I can't sleep\" or \"I have trouble sleeping.\" To get the full picture we can look at words preceding and following the symptom keyword. \n",
    "\n",
    "_ _ _ sleep\n",
    "\n",
    "sleep _ _ _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting different forms of the symptom keywords\n",
    "\n",
    "We can increase chances of finding the right symptoms just by changing the form of the word used. Ex. \"sleepy\" to \"sleepiness\". But there are currently no libraries that can do this accurately and coding this up would take a lot of time which is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sleepe',\n",
       " 'sleeped',\n",
       " 'sleepes',\n",
       " 'sleeping',\n",
       " 'sleepful',\n",
       " 'sleepness',\n",
       " 'sleepfulness']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all forms of the extracted symptoms\n",
    "# i.e. 'sleep' --> 'sleepy', 'sleepiness', 'sleep'\n",
    "#\n",
    "# !pip install reversestem\n",
    "\n",
    "from reversestem import unstem\n",
    "\n",
    "unstem('sleep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Classification models used to match an input list of symptoms to the disease.\n",
    "\n",
    "Following code inspired by this github repo: https://github.com/rahul15197/Disease-Detection-based-on-Symptoms?source=post_page-----54e6be60a3d1--------------------------------\n",
    "\n",
    "We used the dataset from the github repo as well as some general structure of code. Most of the code was written by us to suit our overall structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists used for performance plots\n",
    "\n",
    "model_list = []\n",
    "f1_scores = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "symptoms_dataset = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "\n",
    "# separate the features and labels\n",
    "X = symptoms_dataset.iloc[:, 1:]\n",
    "Y = symptoms_dataset.iloc[:, 0:1]\n",
    "\n",
    "# convert Y (labels) to a 1D array \n",
    "y_array = Y.to_numpy() if isinstance(Y, pd.Series) else Y.values\n",
    "Y = y_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.10)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model according to its type and make predictions\n",
    "def fit_and_predict(classifier, model_type):\n",
    "    classifier = classifier.fit(X, Y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    precision, recall, f1, accuracy = evaluate_model(y_test, y_pred, model_type)\n",
    "    print_evaluation_metrics(precision, recall, f1, accuracy)\n",
    "\n",
    "# Evaluate the model on different metrics\n",
    "def evaluate_model(y_test, y_pred, model_type):\n",
    "    # Calculate precision, recall, and F1-score for each class and then compute the macro average\n",
    "    precision = round(Decimal(precision_score(y_test, y_pred, average='macro', zero_division=1) * 100), 2)\n",
    "    recall= round(Decimal(recall_score(y_test, y_pred, average='macro', zero_division=1) * 100), 2)\n",
    "    f1= round(Decimal(f1_score(y_test, y_pred, average='macro') * 100), 2)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = round(Decimal(accuracy_score(y_test, y_pred) * 100), 2)\n",
    "    \n",
    "    # Add metrics to overall arrays used later for graphing\n",
    "    model_list.append(model_type)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "    \n",
    "def print_evaluation_metrics(precision, recall, f1, accuracy):\n",
    "    print(f'Macro-average Precision: {precision}%')\n",
    "    print(f'Macro-average Recall: {recall}%')\n",
    "    print(f'Macro-average F1-score: {f1}%')\n",
    "    print(f'Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints all model names and their f1 score and accuracy\n",
    "def print_all_metrics():\n",
    "    for i in range(len(model_list)):\n",
    "        print(f'Model: {model_list[i]}, f1 = {f1_scores[i]} accuracy = {accuracies[i]}')\n",
    "        # print(model_list[i], f1_scores[i], accuracies[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "fit_and_predict(mnb, 'MultinomialNaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "fit_and_predict(svm, 'SupportVectorMachine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine classifier\n",
    "lr = LogisticRegression()\n",
    "fit_and_predict(lr, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric lists for reference\n",
    "# model_list = []\n",
    "# f1_scores = []\n",
    "# accuracies = []\n",
    "\n",
    "# Comparison plot for all classifiers with their accuracy\n",
    "plt.style.use('_classic_test_patch')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "plt.title(\"Model Vs Accuracy\", color='black', pad=30)\n",
    "plt.xlabel('Classifier', color='black')\n",
    "plt.ylabel('Accuracy (%)', color='black')\n",
    "plt.bar(model_list, accuracies, color='lightblue')\n",
    "for i, j in enumerate(accuracies):\n",
    "    ax.text(float(i)-0.15, float(j)+0.7, str(j), color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison plot for all classifiers with their F1-score\n",
    "plt.style.use('_classic_test_patch')\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "plt.title(\"Model Vs F1-score\", color='black', pad=30)\n",
    "plt.xlabel('Classifier', color='black')\n",
    "plt.ylabel('F1-score (%)', color='black')\n",
    "plt.bar(model_list, f1_scores, color='lightblue')\n",
    "for i, j in enumerate(f1_scores):\n",
    "    ax.text(float(i)-0.15, float(j)+0.7, str(j), color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedicalAnalyzer for Disease Output\n",
    "\n",
    "This is the main user-model interactive section, all the above code needs to be run so that the following works. This is the minimalist and to the point interaction between user and our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "The best performing model was Logistic Regression, so we'll use that in our evaluations of user input.\n",
    "\n",
    "This code almost completely adapted from: https://github.com/rahul15197/Disease-Detection-based-on-Symptoms/blob/master/SymptomSuggestion.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_comb = pd.read_csv(\"Dataset/dis_sym_dataset_comb.csv\")\n",
    "df_norm = pd.read_csv(\"Dataset/dis_sym_dataset_norm.csv\")\n",
    "\n",
    "# separate the features and labels\n",
    "X = df_comb.iloc[:, 1:]\n",
    "Y = df_comb.iloc[:, 0:1]\n",
    "\n",
    "# convert Y (labels) to a 1D array \n",
    "y_array = Y.to_numpy() if isinstance(Y, pd.Series) else Y.values\n",
    "Y = y_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X, Y)\n",
    "scores = cross_val_score(lr, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_norm.iloc[:, 1:]\n",
    "Y = df_norm.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of symptoms\n",
    "dataset_symptoms = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Symptoms --> Disease\n",
    "\n",
    "This is where we use the model to tell us what diseases the user may have based on the symptoms extracted from their input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector from processed user symptoms to be used by the model\n",
    "\n",
    "# example symptom list\n",
    "# sym_list = [\"yellowish skin\",\"wheezing\",\"abdominal cramp\",\"back\",\"feeling cold\"]\n",
    "\n",
    "def create_symptom_vector(sym_list):\n",
    "    sym_vector = [0 for i in range(0, len(dataset_symptoms))]\n",
    "    for sym in sym_list:\n",
    "        # print(sym)\n",
    "        sym_vector[dataset_symptoms.index(sym)] = 1\n",
    "    return sym_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the likelihood for each disease\n",
    "def predict_diseases(sym_vector):\n",
    "    disease_predict = lr.predict_proba([sym_vector])\n",
    "    k = 10\n",
    "    diseases = list(set(Y['label_dis']))\n",
    "    diseases.sort()\n",
    "    top_k = disease_predict[0].argsort()[-k:][::-1]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_top_k_diseases(top_k):\n",
    "    print(f\"\\nTop {k} diseases predicted based on symptoms\")\n",
    "    topk_dict = {}\n",
    "    # Show top 10 highly probable disease to the user.\n",
    "    for idx,t in  enumerate(top_k):\n",
    "        match_sym=set()\n",
    "        row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
    "        row[0].pop(0)\n",
    "    \n",
    "        for idx,val in enumerate(row[0]):\n",
    "            if val!=0:\n",
    "                match_sym.add(dataset_symptoms[idx])\n",
    "        prob = (len(match_sym.intersection(set(sym_list)))+1)/(len(set(sym_list))+1)\n",
    "        prob *= np.mean(scores)\n",
    "        topk_dict[t] = prob\n",
    "    j = 0\n",
    "    topk_index_mapping = {}\n",
    "    topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    for key in topk_sorted:\n",
    "      prob = topk_sorted[key]*100\n",
    "      print(str(j+1) + \" Disease name:\",diseases[key], \"\\tProbability:\",str(round(prob, 2))+\"%\")\n",
    "      topk_index_mapping[j] = key\n",
    "      j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-interactable Interface\n",
    "\n",
    "Note: In actual implementation the user wouldn't get the list of diseases shown to them, it would instead be saved to a file and shown to the doctor. This implementation is created to show all of the output in one place to see how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "# user_input = \"I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes. I have a fever.\"\n",
    "# new_user_input = normalize_user_input(user_input)\n",
    "def MedicalAnalyzer():\n",
    "    new_user_input = user_input()\n",
    "    print()\n",
    "    \n",
    "    # Extract symptoms\n",
    "    symptoms = extract_symptoms(new_user_input)\n",
    "    \n",
    "    # Clean and join the tokens into keywords\n",
    "    keywords = clean_keywords(symptoms)\n",
    "    \n",
    "    # Display the symptoms\n",
    "    print(\"Key symptom words: \", keywords)\n",
    "    print()\n",
    "    \n",
    "    # Display all symptoms including synonyms\n",
    "    synonyms = get_synonyms(keywords)\n",
    "    final_symptoms = list(synonyms)\n",
    "    print(\"Final symptoms list: \", final_symptoms)\n",
    "    \n",
    "    # Get relevant symptoms\n",
    "    extract_relevant_symptoms(final_symptoms)\n",
    "    \n",
    "    # Create a vector to be used by the model\n",
    "    sym_vector = create_symptom_vector(sym_list)\n",
    "    \n",
    "    # Get top diseases\n",
    "    topk_diseases = predict_diseases(sym_vector)\n",
    "    \n",
    "    output_top_k_diseases(topk_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter symptoms: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I have been having migraines and headaches. I can't sleep. My whole body is shaking and shivering. I feel dizzy sometimes. I have a fever.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key symptom words:  ['migraine', 'headache', 'cannot', 'sleep', 'shaking', 'shivering', 'dizzy', 'fever']\n",
      "\n",
      "Final symptoms list:  ['throw_off', 'migraine', 'kip', 'shake_off', 'judder', 'lightheaded', 'headache', 'hemicrania', 'sleep', 'sick_headache', 'sway', 'head_ache', 'light-headed', 'megrim', 'didder', 'quivering', 'dizzy', 'quiver', 'fever', 'cephalalgia', 'escape_from', 'silly', 'throb', 'woozy', 'palpitation', 'vexation', \"catch_some_Z's\", 'trembling', 'rest', 'eternal_rest', 'shake_up', 'febrility', 'chill', 'empty-headed', 'stimulate', 'vertiginous', 'eternal_sleep', 'shakiness', 'nap', 'featherbrained', 'pyrexia', 'shaking', 'giddy', 'shudder', 'rock', 'quietus', 'shivering', 'airheaded', 'stir', 'thrill', 'sopor', 'febricity', 'concern', 'agitate', 'shaky', 'excite', 'shiver', \"log_Z's\", 'worry', 'vibration', 'slumber', 'shake', 'feverishness']\n",
      "\n",
      "Top 10 diseases predicted based on symptoms\n",
      "1 Disease name: Yellow Fever \tProbability: 44.6%\n",
      "2 Disease name: Parkinson's Disease \tProbability: 29.73%\n",
      "3 Disease name: Lead poisoning \tProbability: 29.73%\n",
      "4 Disease name: Heat-Related Illnesses and Heat waves \tProbability: 29.73%\n",
      "5 Disease name: Influenza \tProbability: 29.73%\n",
      "6 Disease name: Brain Tumour \tProbability: 29.73%\n",
      "7 Disease name: Anxiety \tProbability: 29.73%\n",
      "8 Disease name: Iritis \tProbability: 29.73%\n",
      "9 Disease name: Ebola \tProbability: 29.73%\n",
      "10 Disease name: Migraine \tProbability: 29.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MedicalAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
